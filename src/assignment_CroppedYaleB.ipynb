{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP5328 - Advanced Machine Learning\n",
    "## Assignment 1: Non-negative Matrix Factorization\n",
    "----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Semester 2, 2020)**\n",
    "\n",
    "In this ipython notebook, we provide some example code for assignment1.\n",
    "+ Load Data.\n",
    "    - ORL dataset. \n",
    "    - Extended YaleB dataset. \n",
    "    - AR dataset (**optional**).\n",
    "+ Perform Evaluation. \n",
    "   - Relative Reconstruction Errors.\n",
    "   - Accuracy, NMI (**optional**).\n",
    "\n",
    "Lecturer: Tongliang Liu.\n",
    "\n",
    "Tutors: Nicholas James, Songhua Wu, Xuefeng Li, Yu Yao.\n",
    "\n",
    "**Note: All datasets can be used only for this assignment and you are not allowed to distribute these datasets. If you want to use AR dataset, you need to apply it by yourself (we do not provide AR dataset due to the problem of license, please find more details in http://www2.ece.ohio-state.edu/~aleix/ARdatabase.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload for modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Define your functions in organised individual python files. Don't throw them randomly in the notebook\n",
    "\n",
    "## Import image processing modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd # Used for simpler processing of data\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "\n",
    "## Import Algorithms\n",
    "from algs import NmfHyperEstimator, NmfL2Estimator, ModifiedNMF\n",
    "\n",
    "#Import preprocessing\n",
    "from preprocessing import SaltNPepper, Gaussian, ImageNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "### 1.0 Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# The structure of data folder.\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tree structure of data folder.\n",
    "├── CroppedAR\n",
    "    ├── M-001-01.bmp\n",
    "    ├── M-001-01.txt\n",
    "    ├── M-001-02.bmp\n",
    "    ├── M-001-02.txt\n",
    "    ├── ...\n",
    "├── CroppedYaleB\n",
    "│   ├── yaleB01\n",
    "│   ├── yaleB02\n",
    "│   ...\n",
    "│   ├── yaleB38\n",
    "│   └── yaleB39\n",
    "└── ORL\n",
    "    ├── s1\n",
    "    ├── s2\n",
    "    ├── s3\n",
    "    ├── ...\n",
    "    ├── s40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load ORL Dataset and Extended YaleB Dataset.\n",
    "+ ORL dataset contains ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). The size of each image is 92x112 pixels, with 256 grey levels per pixel. To further reduce the computation complexity, you can resize all images to 30x37 pixels.\n",
    "\n",
    "+ Extended YaleB dataset contains 2414 images of 38 human subjects under 9 poses and 64 illumination conditions. All images are manually aligned, cropped, and then resized to 168x192 pixels. To further reduce the computation complexity, you can resize all images to 42x48 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Extended YaleB dataset.\n",
    "X_yaleb, Y_yaleb = load_data(root='data/CroppedYaleB', reduce=4)\n",
    "print('Extended YalB dataset: X.shape = {}, Y.shape = {}'.format(X_yaleb.shape, Y_yaleb.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Metrics\n",
    "\n",
    "\n",
    "### 2.1 Relative Reconstruction Errors (RRE)\n",
    "\n",
    "To compare the robustness of different NMF algorithms, you can use the ```relative reconstruction errors```. Let $V$ denote the contaminated dataset (by adding noise), and $\\hat{V}$\n",
    " denote the clean dataset. Let $W$ and $H$ denote the factorization results on $V$, the ``relative reconstruction errors`` then can be defined as follows:\n",
    " \\begin{equation}\n",
    "    RRE = \\frac{ \\| \\hat{V} - WH \\|_F }{ \\| \\hat{V} \\|_F}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demonstrate noisy Image\n",
    "V_hat, Y_hat = load_data(root='data/CroppedYaleB', reduce=4)\n",
    "scaler = ImageNormalizer(min=None, max=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = SaltNPepper(p=0.1, r=0.4) \n",
    "\n",
    "V_snp = noise.fit_transform(V_hat)\n",
    "V_snp = scaler.fit_transform(V_snp)\n",
    "\n",
    "noise_g = Gaussian(mean=0, sigma=10) \n",
    "V_g = noise_g.fit_transform(V_hat)\n",
    "V_g = scaler.fit_transform(V_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot result.\n",
    "img_size = [i//4 for i in (168, 192)]\n",
    "ind = 2 # index of demo image.\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(V_hat[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image(Original)')\n",
    "plt.subplot(132)\n",
    "plt.imshow(V_snp[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image(snp Noise) p=0.1 r=0.4')\n",
    "plt.subplot(133)\n",
    "plt.imshow(V_g[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image(Gaussian noise) mean=0 sd=10')\n",
    "plt.draw()\n",
    "plt.savefig(\"noisyImage\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Reconstruction\n",
    "#Using Hypersurface\n",
    "np.random.seed(0)\n",
    "nmf_hyper = NmfHyperEstimator(n_components=50) # set n_components to num_classes.\n",
    "#Salt n Pepper\n",
    "H = nmf_hyper.fit_transform(V_snp)\n",
    "W = nmf_hyper.components_\n",
    "V_snp_reconstructed_hyper = W @ H\n",
    "#Gaussian\n",
    "H = nmf_hyper.fit_transform(V_g)\n",
    "V_g_reconstructed_hyper = W @ H\n",
    "\n",
    "#Using L2NMF\n",
    "np.random.seed(0)\n",
    "nmf_L2 = NmfL2Estimator(n_components=50) # set n_components to num_classes.\n",
    "#SaltNPepper\n",
    "H = nmf_L2.fit_transform(V_snp)\n",
    "W = nmf_L2.components_\n",
    "V_snp_reconstructed_L2 = W @ H\n",
    "#Gaussian\n",
    "H = nmf_L2.fit_transform(V_g)\n",
    "V_g_reconstructed_L2 = W @ H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction (snp noise) using hyper\n",
    "img_size = [i//4 for i in (168, 192)]\n",
    "ind = 2 # index of demo image.\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(V_hat[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (Original)')\n",
    "plt.subplot(132)\n",
    "plt.imshow(V_snp[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (snp Noise) p=0.1 r=0.4')\n",
    "plt.subplot(133)\n",
    "plt.imshow(V_snp_reconstructed_hyper[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (Reconstructed)')\n",
    "plt.savefig(\"reconstructedHyper_snp.png\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction (Gaussian noise) using hyper\n",
    "img_size = [i//4 for i in (168, 192)]\n",
    "ind = 2 # index of demo image.\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(V_hat[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (Original)')\n",
    "plt.subplot(132)\n",
    "plt.imshow(V_g[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (Gaussian noise) mean=20, sd=10')\n",
    "plt.subplot(133)\n",
    "plt.imshow(V_g_reconstructed_hyper[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (Reconstructed)')\n",
    "plt.savefig(\"reconstructedHyper_gaussian\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction (snp noise) using L2NMF\n",
    "img_size = [i//4 for i in (168, 192)]\n",
    "ind = 2 # index of demo image.\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(V_hat[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (Original)')\n",
    "plt.subplot(132)\n",
    "plt.imshow(V_snp[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (snp Noise) p=0.1 r=0.4')\n",
    "plt.subplot(133)\n",
    "plt.imshow(V_snp_reconstructed_L2[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (Reconstructed)')\n",
    "plt.savefig(\"reconstructedL2_snp\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction (Gaussian noise) using L2NMF\n",
    "img_size = [i//4 for i in (168, 192)]\n",
    "ind = 2 # index of demo image.\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(V_hat[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (Original)')\n",
    "plt.subplot(132)\n",
    "plt.imshow(V_g[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image(Gaussian noise) mean=20, sd=10')\n",
    "plt.subplot(133)\n",
    "plt.imshow(V_g_reconstructed_L2[:,ind].reshape(img_size[1],img_size[0]), cmap=plt.cm.gray)\n",
    "plt.title('Image (Reconstructed)')\n",
    "plt.savefig(\"reconstructedL2_gaussian\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Evaluate Clustering Performance\n",
    "\n",
    "1. Accuracy.\n",
    "    \n",
    "    $$ Acc(Y, Y_{pred}) = \\frac{1}{n}\\sum\\limits_{i=1}^n 1\\{Y_{pred}(i) == Y(i)\\}$$\n",
    "        \n",
    "2. Normalized Mutual Information (NMI).\n",
    "\n",
    "    $$ NMI(Y, Y_{pred}) = \\frac{2 * I(Y, Y_{pred})}{H(Y) + H(Y_{pred})} $$\n",
    "    \n",
    "   where $ I(\\cdot,\\cdot) $ is mutual information and $ H(\\cdot) $ is entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "def assign_cluster_label(X, Y):\n",
    "    kmeans = KMeans(n_clusters=len(set(Y))).fit(X)\n",
    "    Y_pred = np.zeros(Y.shape)\n",
    "    for i in set(kmeans.labels_):\n",
    "        ind = kmeans.labels_ == i\n",
    "        Y_pred[ind] = Counter(Y[ind]).most_common(1)[0][0] # assign label.\n",
    "    return Y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison (with scaling) with 90% samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, scaler = X_yaleb, Y_yaleb, ImageNormalizer(min=0, max=1)\n",
    "\n",
    "alg_kwargs_pairs = [\n",
    "    ModifiedNMF, \n",
    "    NmfL2Estimator,\n",
    "    NmfHyperEstimator#, {\"max_armijo\": 2, \"alpha0\": 0.3, \"beta0\": 0.5, \"init\": 'random'}] # disable armijo search\n",
    "]\n",
    "\n",
    "metrics = [rre_score, acc_score, nmi_score]\n",
    "metrics_names = [\"rre\", \"acc_score\", \"nmi_score\"]\n",
    "n_trials = 3         #No of random samples considered for training\n",
    "pc_sample = 0.9      #Training with 40% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saltnpepper noise\n",
    "all_n_components = range(30, 60, 20)\n",
    "noise_alg = SaltNPepper\n",
    "noise_levels = np.arange(0, 0.7, 0.1)\n",
    "ratios = [0.5] #np.arange(0.0, 1, 0.5)\n",
    "noise_kwargs_pairs = make_grid_alg_kwargs(noise_alg, p=noise_levels, r=ratios)\n",
    "\n",
    "evaluations = benchmark(X_yaleb, Y_yaleb, scaler,\n",
    "                        alg_kwargs_pairs, all_n_components,\n",
    "                        noise_kwargs_pairs,\n",
    "                        metrics, metrics_names,\n",
    "                        n_trials, pc_sample\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple check for scores\n",
    "(evaluations.query(\"alg == 'NmfHyperEstimator' and n_components == 30 and r==0.5\")\n",
    " .sort_values([\"rre\"]))#, \"acc_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting RRE, Accuracy and NMI scores against different Salt & pepper noise levels for n_components=50\n",
    "fig = plt.figure(figsize=[9, 12])\n",
    "\n",
    "ax = fig.add_subplot(311)\n",
    "sns.lineplot(\n",
    "    x=\"p\",\n",
    "    y=\"rre\",\n",
    "    hue=\"alg\",\n",
    "    #size=\"n_components\",\n",
    "    style=\"alg\",\n",
    "    data=evaluations.query(\"n_components == 50\"),\n",
    "    estimator='mean',\n",
    "    ci=95,\n",
    "    n_boot=1000,\n",
    "    seed=None,\n",
    "    markers=True,\n",
    "    sort=True,\n",
    "    err_style='band',\n",
    "    #legend='full',\n",
    ")\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel(\"Relative Reconstruction Error\")\n",
    "ax.legend(title=\"Algorithm\", loc=\"upper left\", labels=['NMF (sklearn)','L2-NMF','Hypersurface'])\n",
    "\n",
    "ax = fig.add_subplot(312)\n",
    "sns.lineplot(\n",
    "    x=\"p\",\n",
    "    y=\"acc_score\",\n",
    "    hue=\"alg\",\n",
    "    #size=\"n_components\",\n",
    "    style=\"alg\",\n",
    "    data=evaluations.query(\"n_components == 50\"),# and alg == 'NmfHyperEstimator'\"),\n",
    "    estimator='mean',\n",
    "    ci=95,\n",
    "    n_boot=1000,\n",
    "    seed=None,\n",
    "    markers=True,\n",
    "    sort=True,\n",
    "    err_style='band',\n",
    "    legend='full',\n",
    ")\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel(\"Accuracy Score\")\n",
    "ax.legend(title=\"Algorithm\", loc=\"upper right\", labels=['NMF (sklearn)','L2-NMF','Hypersurface'])\n",
    "\n",
    "ax = fig.add_subplot(313)\n",
    "sns.lineplot(\n",
    "    x=\"p\",\n",
    "    y=\"nmi_score\",\n",
    "    hue=\"alg\",\n",
    "    #size=\"n_components\",\n",
    "    style=\"alg\",\n",
    "    data=evaluations.query(\"n_components == 50\"),# and noise_level==0.1\"),# and alg == 'NmfHyperEstimator'\"),\n",
    "    estimator='mean',\n",
    "    ci=95,\n",
    "    markers=True,\n",
    "    n_boot=1000,\n",
    "    seed=None,\n",
    "    sort=True,\n",
    "    err_style='band',\n",
    "    legend='full',\n",
    ")\n",
    "ax.set_xlabel(\"Noise level\")\n",
    "ax.set_ylabel(\"Normalised Mutual Info Score\")\n",
    "ax.legend(title=\"Algorithm\", loc=\"upper right\", labels=['NMF (sklearn)','L2-NMF','Hypersurface'])\n",
    "plt.title(\"Model performances with n_components=50\")\n",
    "plt.savefig(\"snp_scaled_40%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison (without scaling) with 90% samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, scaler = X_yaleb, Y_yaleb, ImageNormalizer(min=None,max=None)\n",
    "\n",
    "alg_kwargs_pairs = [\n",
    "    ModifiedNMF, \n",
    "    NmfL2Estimator, \n",
    "    NmfHyperEstimator\n",
    "]\n",
    "\n",
    "metrics = [rre_score, acc_score, nmi_score]\n",
    "metrics_names = [\"rre\", \"acc_score\", \"nmi_score\"]\n",
    "n_trials = 3\n",
    "pc_sample = 0.9\n",
    "\n",
    "#Saltnpepper noise\n",
    "all_n_components = [50]\n",
    "noise_alg = SaltNPepper\n",
    "noise_levels = np.arange(0, 0.5, 0.1)\n",
    "ratios = [0.5] #np.arange(0.0, 1, 0.5)\n",
    "noise_kwargs_pairs = make_grid_alg_kwargs(noise_alg, p=noise_levels, r=ratios)\n",
    "\n",
    "evaluations_noscale = benchmark(X_yaleb, Y_yaleb, scaler,\n",
    "                        alg_kwargs_pairs, all_n_components,\n",
    "                        noise_kwargs_pairs,\n",
    "                        metrics, metrics_names,\n",
    "                        n_trials, pc_sample\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting RRE, Accuracy and NMI scores against different Salt & pepper noise levels for n_components=50\n",
    "fig = plt.figure(figsize=[9, 12])\n",
    "\n",
    "ax = fig.add_subplot(311)\n",
    "sns.lineplot(\n",
    "    x=\"p\",\n",
    "    y=\"rre\",\n",
    "    hue=\"alg\",\n",
    "    #size=\"n_components\",\n",
    "    style=\"alg\",\n",
    "    data=evaluations_noscale.query(\"n_components == 50\"),\n",
    "    estimator='mean',\n",
    "    ci=95,\n",
    "    n_boot=1000,\n",
    "    markers=True,\n",
    "    seed=None,\n",
    "    sort=True,\n",
    "    err_style='band',\n",
    "    #legend='full',\n",
    ")\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel(\"Relative Reconstruction Error\")\n",
    "ax.legend(title=\"Algorithm\", loc=\"upper left\", labels=['NMF (sklearn)','L2-NMF','Hypersurface'])\n",
    "\n",
    "ax = fig.add_subplot(312)\n",
    "sns.lineplot(\n",
    "    x=\"p\",\n",
    "    y=\"acc_score\",\n",
    "    hue=\"alg\",\n",
    "    #size=\"n_components\",\n",
    "    style=\"alg\",\n",
    "    data=evaluations_noscale.query(\"n_components == 50\"),# and alg == 'NmfHyperEstimator'\"),\n",
    "    estimator='mean',\n",
    "    ci=95,\n",
    "    n_boot=1000,\n",
    "    markers=True,\n",
    "    seed=None,\n",
    "    sort=True,\n",
    "    err_style='band',\n",
    "    legend='full',\n",
    ")\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel(\"Accuracy Score\")\n",
    "ax.legend(title=\"Algorithm\", loc=\"upper right\", labels=['NMF (sklearn)','L2-NMF','Hypersurface'])\n",
    "\n",
    "ax = fig.add_subplot(313)\n",
    "sns.lineplot(\n",
    "    x=\"p\",\n",
    "    y=\"nmi_score\",\n",
    "    hue=\"alg\",\n",
    "    #size=\"n_components\",\n",
    "    style=\"alg\",\n",
    "    data=evaluations_noscale.query(\"n_components == 50\"),# and noise_level==0.1\"),# and alg == 'NmfHyperEstimator'\"),\n",
    "    estimator='mean',\n",
    "    ci=95,\n",
    "    markers=True,\n",
    "    n_boot=1000,\n",
    "    seed=None,\n",
    "    sort=True,\n",
    "    err_style='band',\n",
    "    legend='full',\n",
    ")\n",
    "ax.set_xlabel(\"Noise level\")\n",
    "ax.set_ylabel(\"Normalised Mutual Info Score\")\n",
    "ax.legend(title=\"Algorithm\", loc=\"upper right\", labels=['NMF (sklearn)','L2-NMF','Hypersurface'])\n",
    "plt.title(\"Model performances with n_components=50\")\n",
    "plt.savefig(\"snp_scaled_90%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
